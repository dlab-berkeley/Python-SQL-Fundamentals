{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1a0aee",
   "metadata": {},
   "source": [
    "# SQL for Data Analysis  \n",
    "## Advanced Workshop\n",
    "*D‚ÄëLab, UC¬†Berkeley*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7ed73-be41-41e1-a27b-75904a6b04eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Prerequisites for SQL Advanced Workshop</b><br><br>\n",
    "Completion of \"SQL for Data Analysis: Introductory Workshop\" or equivalent experience:\n",
    "<ul>\n",
    "<li>Understanding of basic SQL syntax including SELECT, FROM, WHERE, GROUP BY</li>\n",
    "<li>Familiarity with basic data filtering and sorting in SQL</li>\n",
    "<li>Experience with simple aggregations (COUNT, SUM, AVG)</li>\n",
    "</ul>\n",
    "    \n",
    "These prerequisites ensure participants have the foundational knowledge needed to succeed in learning the more advanced concepts like JOINs, subqueries, CTEs, and window functions covered in the second workshop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528f6e4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Relational Joins](#joins)\n",
    "3. [Subqueries](#subqueries)\n",
    "4. [Common Table Expressions (CTEs)](#ctes)\n",
    "5. [Pivoting and Unpivoting](#pivot)\n",
    "6. [Window Functions](#window)\n",
    "7. [Key Points](#keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5e214-796a-48e2-8af6-5f8e39ddec79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Learning Goals</b><br><br>\n",
    "By the end of this workshop you will be able to:\n",
    "<ul>\n",
    "<li>Combine data from multiple tables using different types of JOINs (INNER, LEFT, SELF).</li>\n",
    "<li>Understand the role of primary and foreign keys in establishing table relationships.</li>\n",
    "<li>Write and use subqueries to break down complex queries with multiple logical steps.</li>\n",
    "<li>Simplify complex queries using Common Table Expressions (CTEs).</li>\n",
    "<li>Transform data between row and column orientations with pivoting and unpivoting techniques.</li>\n",
    "<li>Apply window functions to perform calculations across specified sets of rows.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169faec1-82da-45ef-a578-19438be518d1",
   "metadata": {},
   "source": [
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üìù **Poll:** A Zoom poll to help you learn!<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1eb5e2e4-0c1e-48c0-9fef-029bcf52fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some packages that we will need:\n",
    "\n",
    "from IPython.display import SVG\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad7acb",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1 ¬∑ Introduction \n",
    "In the last workshop we covered many different operations one can perform in a given table. However, going back to the idea of Relational Databases, we often won't have all of the information we need on a single table, and we will need to find a way of cross-referencing the information from two (or more) different tables together. This processing is called \"Joining\", and it is an essential aspect of querying with SQL. \n",
    "\n",
    "Another common procedure is to first modify the data, and then perform some operations in the modified data. This is what Subqueries are used for - they allow us to quickly reference a modified version of the dataset we are querying, or compare information between two tables without joining them. Common Table Expressions are a way of doing many subqueries simultaneously, while keeping things readable and organized.\n",
    "\n",
    "Lastly, we will cover a class of operations called Window Functions, which are some of the most powerful tools in SQL. Instead of performing the same function to all rows, they allow the functions to only be applied to *windows*, which are a subset of the observations that are related in a prespecified way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b63d8",
   "metadata": {},
   "source": [
    "<a id='joins'></a>\n",
    "## 2 ¬∑ Relational¬†Joins \n",
    "**Purpose:** JOINs allow us to combine information from multiple different tables into one. When used with Querying, this also provides a way of retaining only the information required for a particular analysis into a single, organized table, even if originally this information was spread out  \n",
    "\n",
    "**Example:** Say that we have two tables, one recording the information about Managers, and another one with the information about Employees. Each employee produces a certain amount of revenue to the firm, but a higher-up is interested in understanding what Manager had direct employees that produce the most revenue. \n",
    "\n",
    "If we had a table that listed each manager, their respective employees and how much they earned, we could use SUM + GROUP BY to quickly do this analysis. But we don't - the information is stored in two separate tables. This is exactly the context of `JOIN`s - creating a new table combining the information from two (or more) other tables. Here is a quick diagram to understand what is happening:\n",
    "\n",
    "![Database Relationship Diagram](database-relationship-diagram2.svg)\n",
    "---\n",
    "\n",
    "Notice that in this example, employee_id would be the *primary key* of each table - a unique identifier for each row - while the column \"report_to\" on the Employees table serves as a *foreign key* - it serves as a reference to values on the table Managers which can be used to cross-reference information.\n",
    "\n",
    "Notice also that we didn't keep all of the information from each table in the end result. By querying the table after joining, we can decide what to retain, and even perform operations or filtering directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9266c29-4d4f-442d-8eaf-86ecd62d131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Managers DataFrame:\n",
      "   employee_id   name\n",
      "0          101    Bob\n",
      "1          102  Alice\n",
      "2          103  Frank\n",
      "\n",
      "Employees DataFrame:\n",
      "   employee_id   name  reports_to  revenue\n",
      "0          201  Carol         101    62000\n",
      "1          202   Dave         101    58000\n",
      "2          203   Erin         102    55000\n",
      "3          204  James         103    48000\n",
      "\n",
      "Merged Table (SQL Result):\n",
      "  Manager Name Employee Name  Revenue\n",
      "0        Alice          Erin    55000\n",
      "1          Bob         Carol    62000\n",
      "2          Bob          Dave    58000\n",
      "3        Frank         James    48000\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the dataframes for Managers and Employees\n",
    "# Managers DataFrame\n",
    "managers_data = {\n",
    "    'employee_id': [101, 102, 103],\n",
    "    'name': ['Bob', 'Alice', 'Frank']\n",
    "}\n",
    "managers_df = pd.DataFrame(managers_data)\n",
    "\n",
    "# Employees DataFrame\n",
    "employees_data = {\n",
    "    'employee_id': [201, 202, 203, 204],\n",
    "    'name': ['Carol', 'Dave', 'Erin', 'James'],\n",
    "    'reports_to': [101, 101, 102, 103],\n",
    "    'revenue': [62000, 58000, 55000, 48000]\n",
    "}\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"Managers DataFrame:\")\n",
    "print(managers_df)\n",
    "print(\"\\nEmployees DataFrame:\")\n",
    "print(employees_df)\n",
    "\n",
    "# 2. Import dataframes to SQL\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Write DataFrames to SQL\n",
    "managers_df.to_sql('Managers', engine, index=False, if_exists='replace')\n",
    "employees_df.to_sql('Employees', engine, index=False, if_exists='replace')\n",
    "\n",
    "# 3. SQL code to generate the Merged table\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.name AS \"Manager Name\",\n",
    "    e.name AS \"Employee Name\",\n",
    "    e.revenue AS \"Revenue\"\n",
    "FROM \n",
    "    Managers m \n",
    "JOIN \n",
    "    Employees e \n",
    "ON \n",
    "    m.employee_id = e.reports_to\n",
    "ORDER BY\n",
    "    m.name, e.name\n",
    "\"\"\"\n",
    "\n",
    "# 4. Execute the query and print the Merged table\n",
    "print(\"\\nMerged Table (SQL Result):\")\n",
    "merged_df = pd.read_sql_query(query, engine)\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c194c-2a3c-4b7d-a453-95fe9a552a71",
   "metadata": {},
   "source": [
    "Let's Break down exactly what is happening in the Query above:\n",
    "\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    m.name AS \"Manager Name\",\n",
    "    e.name AS \"Employee Name\",\n",
    "    e.revenue AS \"Revenue\"\n",
    "FROM \n",
    "    Managers m \n",
    "JOIN \n",
    "    Employees e \n",
    "ON \n",
    "    m.employee_id = e.reports_to\n",
    "ORDER BY\n",
    "    m.name, e.name\n",
    "```\n",
    "\n",
    "Let's start from the inside out:\n",
    "- The first operation being performed is \"FROM\". This is telling us which Table we will be using as our \"main\" table - which in SQL is usually referred to as the Left Table. It is useful to think of any JOIN operation as acting \"to the right\" of this table.\n",
    "    - Notice that we include a \"m\" right next to the name of the table we are importing - this is what we call an alias. While this is not necessary for a simple JOIN statement like this, it help keep the query clean and organized. And, as we will see later on, it is necessary for more advanced JOIN statements (such as self-joins).\n",
    "- The second operation being performed is \"JOIN\". This is essentially telling SQL to consider a table that is a combination of the Left and Right tables.\n",
    "    - One very important aspect is that we must tell SQL how to merge these two tables - which is what we do by using the ON statement.\n",
    "    - In more advanced queries, ON statements can use very complex conditions, including compound ones using logical operators such as AND/OR/NOT, but for now we are just saying \"I want a row that combines the information from the two tables whenever the column \"employeed_id\" on the Manager table matches the \"reports_to\" column on the Employees table.\n",
    "    - Notice that we had to specify which table each column came from here - and we can already see why aliases can come in handy.\n",
    "    - A very important thing to notice is that, if there are multiple matches to the condition on the ON statement, any rows that satisfy it will be joined. This is what happened in our example - there are two rows in the Merged table that have Bob as the Manager Name, since he had two employees that reported to him.\n",
    "    - Another important thing to know is that, in a basic JOIN statement, any rows that go unmatched are not included in the final result. We will see how more advanced JOIN statements - such as LEFT JOIN - allow us to bypass this.\n",
    "- We then go to our SELECT statement. As before, this is just telling SQL which information we actually want it to retrieve for us. But, since the information now might come from more than one source, we again need to specify from which table each column is coming from.\n",
    "- The rest of the statements is similar to a basic Query - we can filter, order, limit/offset, etc - just like we did in the previous workshop\n",
    "\n",
    "\n",
    "![JOIN Query Flow](sql-query-flow.svg)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10c8202",
   "metadata": {
    "language": "sql",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departments DataFrame:\n",
      "    department  employee_name  employee_id\n",
      "0  Engineering     John Smith          101\n",
      "1    Marketing  Sarah Johnson          102\n",
      "2        Sales    Michael Lee          103\n",
      "3           HR     John Smith          104\n",
      "4  Engineering     Emma Davis          105\n",
      "\n",
      "Salaries DataFrame:\n",
      "   employee_name  employee_id  hours_worked  hourly_salary\n",
      "0     John Smith          101            40          35.50\n",
      "1  Sarah Johnson          102            35          42.00\n",
      "2    Michael Lee          103            38          38.75\n",
      "3     John Smith          104            30          45.25\n",
      "4   David Wilson          106            25          33.00\n",
      "\n",
      " JOIN Results:\n",
      "   employee_name   department  total_salary\n",
      "0     John Smith  Engineering        1420.0\n",
      "1     John Smith           HR        1357.5\n",
      "2  Sarah Johnson    Marketing        1470.0\n",
      "3    Michael Lee        Sales        1472.5\n"
     ]
    }
   ],
   "source": [
    "# IDEA FOR CHALLENGE 1: Produce a table which tells us the employee name, their department, and their total salary - in this order\n",
    "\n",
    "\n",
    "# Create Departments dataframe\n",
    "departments_data = {\n",
    "    'department': ['Engineering', 'Marketing', 'Sales', 'HR', 'Engineering'],\n",
    "    'employee_name': ['John Smith', 'Sarah Johnson', 'Michael Lee', 'John Smith', 'Emma Davis'],\n",
    "    'employee_id': [101, 102, 103, 104, 105]\n",
    "}\n",
    "departments_df = pd.DataFrame(departments_data)\n",
    "\n",
    "# Create Salaries dataframe\n",
    "salaries_data = {\n",
    "    'employee_name': ['John Smith', 'Sarah Johnson', 'Michael Lee', 'John Smith', 'David Wilson'],\n",
    "    'employee_id': [101, 102, 103, 104, 106],\n",
    "    'hours_worked': [40, 35, 38, 30, 25],\n",
    "    'hourly_salary': [35.50, 42.00, 38.75, 45.25, 33.00]\n",
    "}\n",
    "salaries_df = pd.DataFrame(salaries_data)\n",
    "\n",
    "print(\"Departments DataFrame:\")\n",
    "print(departments_df)\n",
    "print(\"\\nSalaries DataFrame:\")\n",
    "print(salaries_df)\n",
    "\n",
    "# Note the key aspects:\n",
    "# - Two employees named \"John Smith\" (employee_id 101 and 104)\n",
    "# - \"Emma Davis\" (id 105) is in Departments but not in Salaries\n",
    "# - \"David Wilson\" (id 106) is in Salaries but not in Departments\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Import the dataframes to SQL tables\n",
    "departments_df.to_sql('Departments', engine, index=False)\n",
    "salaries_df.to_sql('Salaries', engine, index=False)\n",
    "\n",
    "# Different types of joins\n",
    "\n",
    "# Inner join (only matching records)\n",
    "inner_join_query = \"\"\"\n",
    "SELECT\n",
    "    d.employee_name,\n",
    "    d.department,\n",
    "    s.hours_worked * s.hourly_salary AS total_salary\n",
    "FROM \n",
    "    Departments d\n",
    "INNER JOIN \n",
    "    Salaries s\n",
    "ON \n",
    "    d.employee_id = s.employee_id\n",
    "ORDER BY\n",
    "    d.department, d.employee_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n JOIN Results:\")\n",
    "inner_join_results = pd.read_sql_query(inner_join_query, engine)\n",
    "print(inner_join_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b1c6d-555b-4fa2-a179-64c9463f4655",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Always make sure that your joining condition is present and accurate. Otherwise SQL will perform what we call a Cartesian Product - making every possible combination between the rows - which can become huge really fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b9dd7",
   "metadata": {},
   "source": [
    "## 2.1¬†`ON` versus `USING`\n",
    "While the ON clause is extremely flexible, if the two tables share a column, we can use the simpler USING() method.\n",
    "\n",
    "‚ö†Ô∏è  **Warning:** When using \"USING\" to JOIN tables, always make sure that the names are identical in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d969b9",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " USING() JOIN Results:\n",
      "   employee_name   department  total_salary\n",
      "0     John Smith  Engineering        1420.0\n",
      "1  Sarah Johnson    Marketing        1470.0\n",
      "2    Michael Lee        Sales        1472.5\n",
      "3     John Smith           HR        1357.5\n"
     ]
    }
   ],
   "source": [
    "using_join_query = \"\"\"\n",
    "SELECT\n",
    "    d.employee_name,\n",
    "    d.department,\n",
    "    s.hours_worked * s.hourly_salary AS total_salary\n",
    "FROM \n",
    "    Departments d\n",
    "INNER JOIN \n",
    "    Salaries s\n",
    "USING(employee_id)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n USING() JOIN Results:\")\n",
    "using_join_results = pd.read_sql_query(using_join_query, engine)\n",
    "print(using_join_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d6294-f9ec-4cef-929f-23c10ec5f100",
   "metadata": {},
   "source": [
    "But is important to be careful - using a non-primary key column can lead to issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "158883ad-c959-4172-a097-ba763b526d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Problematic USING() JOIN Results:\n",
      "   employee_name   department  total_salary\n",
      "0     John Smith  Engineering        1357.5\n",
      "1     John Smith  Engineering        1420.0\n",
      "2  Sarah Johnson    Marketing        1470.0\n",
      "3    Michael Lee        Sales        1472.5\n",
      "4     John Smith           HR        1357.5\n",
      "5     John Smith           HR        1420.0\n"
     ]
    }
   ],
   "source": [
    "using_join_query_2 = \"\"\"\n",
    "SELECT\n",
    "    d.employee_name,\n",
    "    d.department,\n",
    "    s.hours_worked * s.hourly_salary AS total_salary\n",
    "FROM \n",
    "    Departments d\n",
    "INNER JOIN \n",
    "    Salaries s\n",
    "USING(employee_name)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Problematic USING() JOIN Results:\")\n",
    "using_join_results_2 = pd.read_sql_query(using_join_query_2, engine)\n",
    "print(using_join_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142be246-ebcc-43ea-bc9d-1e2a163ad391",
   "metadata": {},
   "source": [
    "üîî **Question:** Can anyone figure out what went wrong here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf25be",
   "metadata": {},
   "source": [
    "## 2.2¬†Multiple¬†Joins\n",
    "We don't have to stop at two - we can use multiple joins at once. And what is interesting is that we can get pretty creative with the ON conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "615bd16e",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees Table:\n",
      "   employee_id            name\n",
      "0          101      John Smith\n",
      "1          102   Sarah Johnson\n",
      "2          103     Michael Lee\n",
      "3          104      Emma Davis\n",
      "4          105    David Wilson\n",
      "5          106  Jennifer Lopez\n",
      "\n",
      "Ranks Table:\n",
      "   employee_id     rank\n",
      "0          101   Senior\n",
      "1          102  Manager\n",
      "2          103   Junior\n",
      "3          104   Senior\n",
      "4          105   Junior\n",
      "5          107  Manager\n",
      "\n",
      "Wages Table:\n",
      "       rank  hourly_wage\n",
      "0    Junior        25.50\n",
      "1    Senior        35.75\n",
      "2   Manager        45.00\n",
      "3  Director        65.00\n",
      "\n",
      "Hours Table:\n",
      "   employee_id  hours_worked\n",
      "0          101            40\n",
      "1          102            45\n",
      "2          103            38\n",
      "3          105            35\n",
      "4          106            42\n",
      "5          108            30\n",
      "\n",
      " Join Results:\n",
      "            name  total_pay\n",
      "0  Sarah Johnson     2025.0\n",
      "1     John Smith     1430.0\n",
      "2    Michael Lee      969.0\n",
      "3   David Wilson      892.5\n"
     ]
    }
   ],
   "source": [
    "# Create the four tables\n",
    "# 1. Employees: employee_id and names\n",
    "employees_data = {\n",
    "    'employee_id': [101, 102, 103, 104, 105, 106],\n",
    "    'name': ['John Smith', 'Sarah Johnson', 'Michael Lee', 'Emma Davis', 'David Wilson', 'Jennifer Lopez']\n",
    "}\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# 2. Employee Ranks: employee_id and their job rank\n",
    "ranks_data = {\n",
    "    'employee_id': [101, 102, 103, 104, 105, 107],  # Note: 107 is not in employees, 106 is missing\n",
    "    'rank': ['Senior', 'Manager', 'Junior', 'Senior', 'Junior', 'Manager']\n",
    "}\n",
    "ranks_df = pd.DataFrame(ranks_data)\n",
    "\n",
    "# 3. Wages: job rank and hourly wage\n",
    "wages_data = {\n",
    "    'rank': ['Junior', 'Senior', 'Manager', 'Director'],  # Note: No employees with \"Director\" rank\n",
    "    'hourly_wage': [25.50, 35.75, 45.00, 65.00]\n",
    "}\n",
    "wages_df = pd.DataFrame(wages_data)\n",
    "\n",
    "# 4. Hours: employee_id and hours worked\n",
    "hours_data = {\n",
    "    'employee_id': [101, 102, 103, 105, 106, 108],  # Note: 108 is not in employees, 104 is missing\n",
    "    'hours_worked': [40, 45, 38, 35, 42, 30]\n",
    "}\n",
    "hours_df = pd.DataFrame(hours_data)\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"Employees Table:\")\n",
    "print(employees_df)\n",
    "print(\"\\nRanks Table:\")\n",
    "print(ranks_df)\n",
    "print(\"\\nWages Table:\")\n",
    "print(wages_df)\n",
    "print(\"\\nHours Table:\")\n",
    "print(hours_df)\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Import the dataframes to SQL tables\n",
    "employees_df.to_sql('Employees', engine, index=False)\n",
    "ranks_df.to_sql('Ranks', engine, index=False)\n",
    "wages_df.to_sql('Wages', engine, index=False)\n",
    "hours_df.to_sql('Hours', engine, index=False)\n",
    "\n",
    "# JOIN\n",
    "inner_join_query = \"\"\"\n",
    "SELECT \n",
    "    e.name,\n",
    "    w.hourly_wage * h.hours_worked AS total_pay\n",
    "FROM \n",
    "    Employees e\n",
    "INNER JOIN \n",
    "    Ranks r ON e.employee_id = r.employee_id\n",
    "INNER JOIN \n",
    "    Wages w ON r.rank = w.rank\n",
    "INNER JOIN \n",
    "    Hours h ON e.employee_id = h.employee_id\n",
    "ORDER BY\n",
    "    total_pay DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Join Results:\")\n",
    "inner_join_results = pd.read_sql_query(inner_join_query, engine)\n",
    "print(inner_join_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d742f-354f-44d7-acd0-4b99a82654e4",
   "metadata": {},
   "source": [
    "üîî **Question:** Why can't we see the total_pay for Jennifer Lopez and Emma Davis?\n",
    "\n",
    "This is an extremely important property of the JOIN command - it is what we call an INNER JOIN.\n",
    "\n",
    "In practice, this means that we only return rows that are present on both tables being merged. So, in the previous example, sinced Jennifer did not have an entry in the Rank table, this entry was dropped. Similarly, since Emma did not have her hours recorded on the Hours table, her entry was dropped on the last JOIN.\n",
    "\n",
    "Of course, there are many situations that this is not what we want. For example, we might want to return NULL for Jennifer Lopez, to indicate that there was a missing record, and 0 for Emma Davis, since she did not work any hours. \n",
    "\n",
    "In this workshop we will be focusing on the two most common advanced methods of JOIN - LEFT JOIN and SELF JOIN - but for those curious, there are others, such as RIGHT JOIN, FULL JOIN AND CROSS JOIN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8d04f",
   "metadata": {},
   "source": [
    "## 2.3¬†Advanced Join Variants\n",
    "- **LEFT JOIN**: retains all rows from the left table.\n",
    "    - This is why it is important to distinguish which table is being used in the FROM statement, and which is being brought by the JOIN statement\n",
    "    - Specially useful to handle missing data \n",
    "- **SELF¬†JOIN**: the table is joined to itself\n",
    "    - We essentially deal with two tables - one of them being a duplicate of the first - and then JOIN them\n",
    "    - Very useful as a filtering tool\n",
    "\n",
    "üîî **Question:** Can anyone think of an example in which we might want to join a table with itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84ccc9db",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees Table:\n",
      "   employee_id            name\n",
      "0          101      John Smith\n",
      "1          102   Sarah Johnson\n",
      "2          103     Michael Lee\n",
      "3          104      Emma Davis\n",
      "4          105    David Wilson\n",
      "5          106  Jennifer Lopez\n",
      "\n",
      "Ranks Table:\n",
      "   employee_id     rank\n",
      "0          101   Senior\n",
      "1          102  Manager\n",
      "2          103   Junior\n",
      "3          104   Senior\n",
      "4          105   Junior\n",
      "5          107  Manager\n",
      "\n",
      "Left Join Results:\n",
      "   employee_id            name  employee_id     rank\n",
      "0          101      John Smith        101.0   Senior\n",
      "1          102   Sarah Johnson        102.0  Manager\n",
      "2          103     Michael Lee        103.0   Junior\n",
      "3          104      Emma Davis        104.0   Senior\n",
      "4          105    David Wilson        105.0   Junior\n",
      "5          106  Jennifer Lopez          NaN     None\n"
     ]
    }
   ],
   "source": [
    "# Create the four tables\n",
    "# 1. Employees: employee_id and names\n",
    "employees_data = {\n",
    "    'employee_id': [101, 102, 103, 104, 105, 106],\n",
    "    'name': ['John Smith', 'Sarah Johnson', 'Michael Lee', 'Emma Davis', 'David Wilson', 'Jennifer Lopez']\n",
    "}\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# 2. Employee Ranks: employee_id and their job rank\n",
    "ranks_data = {\n",
    "    'employee_id': [101, 102, 103, 104, 105, 107],  # Note: 107 is not in employees, 106 is missing\n",
    "    'rank': ['Senior', 'Manager', 'Junior', 'Senior', 'Junior', 'Manager']\n",
    "}\n",
    "ranks_df = pd.DataFrame(ranks_data)\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"Employees Table:\")\n",
    "print(employees_df)\n",
    "print(\"\\nRanks Table:\")\n",
    "print(ranks_df)\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Import the dataframes to SQL tables\n",
    "employees_df.to_sql('Employees', engine, index=False)\n",
    "ranks_df.to_sql('Ranks', engine, index=False)\n",
    "wages_df.to_sql('Wages', engine, index=False)\n",
    "hours_df.to_sql('Hours', engine, index=False)\n",
    "\n",
    "# LEFT JOIN\n",
    "left_join_query = \"\"\"\n",
    "SELECT *\n",
    "FROM \n",
    "    Employees e\n",
    "LEFT JOIN \n",
    "    Ranks r ON e.employee_id = r.employee_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nLeft Join Results:\")\n",
    "left_join_results = pd.read_sql_query(left_join_query, engine)\n",
    "print(left_join_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43d623-1e4a-4720-ba7c-5b171bab3fd3",
   "metadata": {},
   "source": [
    "IDEA FOR CHALLENGE: Let's check that the order is indeed relevant. Let's use a LEFT JOIN between these two table - but now let's use the Rank table as the left one, and the Employees as the right one. What happened? Does that make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4bf71-1050-4c9b-b537-1c52b1a9d5a1",
   "metadata": {},
   "source": [
    "One problem with using LEFT JOIN is that it often returns NONE values - but we might want to interpret them in a different way. For example, we saw that Emma Davis did not work any hours, but if we did a left join on Employees and Hours our result would indicate that she worked NONE hours. \n",
    "\n",
    "Luckily, SQL includes a function that allows us to taylor the behavior of NONE entries for a given query - the COALESCE command. The first argument of the function tells us which column to analyze, and the second entry what to replace NONE values by. Let's take a look at an example in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "277f9147-3448-4122-bb84-ff97bbd8459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees Table:\n",
      "   employee_id            name\n",
      "0          101      John Smith\n",
      "1          102   Sarah Johnson\n",
      "2          103     Michael Lee\n",
      "3          104      Emma Davis\n",
      "4          105    David Wilson\n",
      "5          106  Jennifer Lopez\n",
      "\n",
      "Hours Table:\n",
      "   employee_id  hours_worked\n",
      "0          101            40\n",
      "1          102            45\n",
      "2          103            38\n",
      "3          105            35\n",
      "4          106            42\n",
      "5          108            30\n",
      "\n",
      "LEFT JOIN Results with COALESCE:\n",
      "   employee_id            name  hours_worked\n",
      "0          101      John Smith            40\n",
      "1          102   Sarah Johnson            45\n",
      "2          103     Michael Lee            38\n",
      "3          104      Emma Davis             0\n",
      "4          105    David Wilson            35\n",
      "5          106  Jennifer Lopez            42\n"
     ]
    }
   ],
   "source": [
    "# Create Employees dataframe\n",
    "employees_data = {\n",
    "    'employee_id': [101, 102, 103, 104, 105, 106],\n",
    "    'name': ['John Smith', 'Sarah Johnson', 'Michael Lee', 'Emma Davis', 'David Wilson', 'Jennifer Lopez']\n",
    "}\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# Create Hours dataframe (with some employees missing)\n",
    "hours_data = {\n",
    "    'employee_id': [101, 102, 103, 105, 106, 108],  # Note: 108 is not in employees, 104 is missing\n",
    "    'hours_worked': [40, 45, 38, 35, 42, 30]\n",
    "}\n",
    "hours_df = pd.DataFrame(hours_data)\n",
    "\n",
    "# Display the original dataframes\n",
    "print(\"Employees Table:\")\n",
    "print(employees_df)\n",
    "print(\"\\nHours Table:\")\n",
    "print(hours_df)\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Import the dataframes to SQL tables\n",
    "employees_df.to_sql('Employees', engine, index=False)\n",
    "hours_df.to_sql('Hours', engine, index=False)\n",
    "\n",
    "# LEFT JOIN query with COALESCE to handle NULL hours\n",
    "left_join_query = \"\"\"\n",
    "SELECT \n",
    "    e.employee_id,\n",
    "    e.name,\n",
    "    COALESCE(h.hours_worked, 0) AS hours_worked\n",
    "FROM \n",
    "    Employees e\n",
    "LEFT JOIN \n",
    "    Hours h ON e.employee_id = h.employee_id\n",
    "ORDER BY\n",
    "    e.employee_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "left_join_results = pd.read_sql_query(left_join_query, engine)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nLEFT JOIN Results with COALESCE:\")\n",
    "print(left_join_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37315fe8-7ed7-4c04-85fc-b9a32020ca84",
   "metadata": {},
   "source": [
    "Now let's talk a bit about SELF JOIN. As we mentioned before, the idea here is to join a table with itself. This is one of the cases in which using aliases is extremely important, since, by construction, both of the tables will have all columns in common!\n",
    "\n",
    "Let's see an example on how this might work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "907ffd7a-e45e-4321-b17c-9ca66e7cca2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees Table:\n",
      "   employee_id      name  salary  reports_to\n",
      "0            1      John  150000         NaN\n",
      "1            2     Sarah  120000         1.0\n",
      "2            3   Michael  140000         1.0\n",
      "3            4      Emma  115000         2.0\n",
      "4            5     David  125000         2.0\n",
      "5            6  Jennifer   95000         3.0\n",
      "6            7    Robert  160000         3.0\n",
      "7            8      Lisa   90000         4.0\n",
      "8            9     Kevin   80000         5.0\n",
      "  employee_name  employee_salary manager_name  manager_salary\n",
      "0         Sarah           120000         John          150000\n",
      "1       Michael           140000         John          150000\n",
      "2          Emma           115000        Sarah          120000\n",
      "3         David           125000        Sarah          120000\n",
      "4      Jennifer            95000      Michael          140000\n",
      "5        Robert           160000      Michael          140000\n",
      "6          Lisa            90000         Emma          115000\n",
      "7         Kevin            80000        David          125000\n"
     ]
    }
   ],
   "source": [
    "# Create Employees dataframe with salary and reporting structure\n",
    "employees_data = {\n",
    "    'employee_id': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'name': ['John', 'Sarah', 'Michael', 'Emma', 'David', 'Jennifer', 'Robert', 'Lisa', 'Kevin'],\n",
    "    'salary': [150000, 120000, 140000, 115000, 125000, 95000, 160000, 90000, 80000],\n",
    "    'reports_to': [None, 1, 1, 2, 2, 3, 3, 4, 5]  # None for CEO\n",
    "}\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# Display the original dataframe\n",
    "print(\"Employees Table:\")\n",
    "print(employees_df)\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Import the dataframe to SQL table\n",
    "employees_df.to_sql('Employees', engine, index=False)\n",
    "\n",
    "# Self JOIN query to compare the salaries of employees and their managers\n",
    "self_join_query = \"\"\"\n",
    "SELECT \n",
    "    e.name AS employee_name,\n",
    "    e.salary AS employee_salary,\n",
    "    m.name AS manager_name,\n",
    "    m.salary AS manager_salary\n",
    "FROM \n",
    "    Employees e\n",
    "JOIN \n",
    "    Employees m ON e.reports_to = m.employee_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "results = pd.read_sql_query(self_join_query, engine)\n",
    "\n",
    "# Display the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9bb68-b741-4363-8a54-2e1ae415c1e8",
   "metadata": {},
   "source": [
    "There are a few things worth noticing here:\n",
    "- When we selected the columns from each table, it was important to rename them - since they had the same original names!\n",
    "- Notice that John did not report to anyone - so he was not included as an employee in the merged table. This could be adapted by using a LEFT JOIN\n",
    "- Some managers appear many times, since more than one employee reports to them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d9ff0-f6d2-46ed-8c95-1a26a3a013f4",
   "metadata": {},
   "source": [
    "IDEA FOR CHALLENGE: SELF JOINs can be extremely useful as a filtering tool. A classic example is to return the name of the employees who earn more than their managers. Can you modify the previous query and return **only** the name of the employees who earn more than their managers? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4927941-cc29-4616-881f-09d8000a270a",
   "metadata": {},
   "source": [
    "Of course, SELF JOINs can be used for purposes other than filtering. Another classic application is to find pairs that satisfy some criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f04d4262-d602-4e13-8c2e-ebb1819b64c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valuations Table:\n",
      "      person_name    role  value\n",
      "0      John Smith   buyer    500\n",
      "1   Sarah Johnson  seller    400\n",
      "2     Michael Lee   buyer    350\n",
      "3      Emma Davis  seller    300\n",
      "4    David Wilson   buyer    600\n",
      "5  Jennifer Lopez  seller    550\n",
      "6    Robert Brown   buyer    450\n",
      "7       Lisa Chen  seller    500\n",
      "\n",
      "Pairs where buyer values object more than seller:\n",
      "     buyer_name     seller_name\n",
      "0  David Wilson      Emma Davis\n",
      "1  David Wilson  Jennifer Lopez\n",
      "2  David Wilson       Lisa Chen\n",
      "3  David Wilson   Sarah Johnson\n",
      "4    John Smith      Emma Davis\n",
      "5    John Smith   Sarah Johnson\n",
      "6   Michael Lee      Emma Davis\n",
      "7  Robert Brown      Emma Davis\n",
      "8  Robert Brown   Sarah Johnson\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with people, their roles, and valuations\n",
    "valuations_data = {\n",
    "    'person_name': ['John Smith', 'Sarah Johnson', 'Michael Lee', 'Emma Davis', \n",
    "                    'David Wilson', 'Jennifer Lopez', 'Robert Brown', 'Lisa Chen'],\n",
    "    'role': ['buyer', 'seller', 'buyer', 'seller', 'buyer', 'seller', 'buyer', 'seller'],\n",
    "    'value': [500, 400, 350, 300, 600, 550, 450, 500]\n",
    "}\n",
    "valuations_df = pd.DataFrame(valuations_data)\n",
    "\n",
    "# Display the original dataframe\n",
    "print(\"Valuations Table:\")\n",
    "print(valuations_df)\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Import the dataframe to SQL table\n",
    "valuations_df.to_sql('Valuations', engine, index=False)\n",
    "\n",
    "# Self JOIN query to find buyer-seller pairs where buyer values object more than seller\n",
    "self_join_query = \"\"\"\n",
    "SELECT \n",
    "    b.person_name AS buyer_name,\n",
    "    s.person_name AS seller_name\n",
    "FROM \n",
    "    Valuations b\n",
    "JOIN \n",
    "    Valuations s ON b.role = 'buyer' AND s.role = 'seller'\n",
    "WHERE \n",
    "    b.value > s.value\n",
    "ORDER BY\n",
    "    b.person_name, s.person_name\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and display results\n",
    "results = pd.read_sql_query(self_join_query, engine)\n",
    "print(\"\\nPairs where buyer values object more than seller:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1071b0",
   "metadata": {},
   "source": [
    "<a id='subqueries'></a>\n",
    "## 3¬†¬∑¬†Subqueries \n",
    "\n",
    "**Definition.** As the name says, a subquery is a query contained in another query. This allows us to perform auxiliary queries, and then use the results of these queries in our main query. Unlike the main query, subqueries are temporary - they only exist while the instance of the query is being worked out. \n",
    "\n",
    "**Purpose.** Subqueries are very useful when one needs to break down a complex question into multiple manageable individual parts.For example, one might want to summarize or filter a given table, and use the summarized/filtered results as the input of another query. \n",
    "\n",
    "**Common Use¬†Case.** Getting summary statistics for each individual, while keeping information unrelated from the variable we are using to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8db71b04-8952-42cb-a923-bda8226e739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer totals withOUT all customer information:\n",
      "   customer_id  total_spent\n",
      "0          101         1225\n",
      "1          103          630\n",
      "2          102          350\n",
      "Customer totals with all customer information:\n",
      "   customer_id customer_name customer_city  total_spent\n",
      "0          101         Alice      New York         1225\n",
      "1          103       Charlie   Los Angeles          630\n",
      "2          102           Bob       Chicago          350\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "orders_data = {\n",
    "    'order_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'customer_id': [101, 101, 102, 103, 103],\n",
    "    'product': ['Laptop', 'Mouse', 'Monitor', 'Printer', 'Tablet'],\n",
    "    'amount': [1200, 25, 350, 180, 450]\n",
    "}\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "customers_data = {\n",
    "    'customer_id': [101, 102, 103, 104],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'customer_city': ['New York', 'Chicago', 'Los Angeles', 'Boston']\n",
    "}\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "orders_df.to_sql('Orders', engine, index=False)\n",
    "customers_df.to_sql('Customers', engine, index=False)\n",
    "\n",
    "without_subquery = \"\"\"\n",
    "SELECT customer_id, SUM(amount) as total_spent \n",
    "     FROM Orders \n",
    "     GROUP BY customer_id\n",
    "ORDER BY\n",
    "    total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(without_subquery, engine)\n",
    "print(\"Customer totals withOUT all customer information:\")\n",
    "print(result)\n",
    "\n",
    "# JOIN with a subquery to get customer totals with their information\n",
    "join_on_subquery = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name, \n",
    "    c.customer_city,\n",
    "    s.total_spent\n",
    "FROM \n",
    "    Customers c\n",
    "JOIN \n",
    "    (SELECT customer_id, SUM(amount) as total_spent \n",
    "     FROM Orders \n",
    "     GROUP BY customer_id) s\n",
    "ON \n",
    "    c.customer_id = s.customer_id\n",
    "ORDER BY\n",
    "    total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(join_on_subquery, engine)\n",
    "print(\"Customer totals with all customer information:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d4fe0-ab4a-4038-a0e8-49574684cb0d",
   "metadata": {},
   "source": [
    "There are two main ways of using subqueries:\n",
    "\n",
    "1) To Filter results.\n",
    "\n",
    "A classic example is to find all customers who spend more than average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a013097e-4c6d-479f-b147-74f3f89b9cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers who spend more than the average:\n",
      "   customer_id customer_name  total_spent  average_customer_spend\n",
      "0          101         Alice         1270                  586.25\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "orders_data = {\n",
    "    'order_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007],\n",
    "    'customer_id': [101, 101, 102, 101, 103, 102, 104],\n",
    "    'product': ['Laptop', 'Mouse', 'Monitor', 'Keyboard', 'Printer', 'Headphones', 'Tablet'],\n",
    "    'amount': [1200, 25, 350, 45, 180, 95, 450]\n",
    "}\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "customers_data = {\n",
    "    'customer_id': [101, 102, 103, 104],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "}\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "orders_df.to_sql('Orders', engine, index=False)\n",
    "customers_df.to_sql('Customers', engine, index=False)\n",
    "\n",
    "# Query to find customers who spend more than average\n",
    "above_average_query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    SUM(o.amount) as total_spent,\n",
    "    (SELECT AVG(total) FROM (\n",
    "        SELECT customer_id, SUM(amount) as total\n",
    "        FROM Orders\n",
    "        GROUP BY customer_id\n",
    "    )) as average_customer_spend\n",
    "FROM \n",
    "    Customers c\n",
    "JOIN \n",
    "    Orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY \n",
    "    c.customer_id, c.customer_name\n",
    "HAVING \n",
    "    SUM(o.amount) > (\n",
    "        SELECT AVG(total) FROM (\n",
    "            SELECT customer_id, SUM(amount) as total\n",
    "            FROM Orders\n",
    "            GROUP BY customer_id\n",
    "        )\n",
    "    )\n",
    "ORDER BY\n",
    "    total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(above_average_query, engine)\n",
    "print(\"Customers who spend more than the average:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaab118-4ade-400c-8b7e-31666ee71539",
   "metadata": {},
   "source": [
    "2) As a derived table to query from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eb7cad2-619f-402c-b6df-7e9ff50b6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees Data:\n",
      "   employee_id employee_name  department_id  salary\n",
      "0            1    Employee_1              1   45000\n",
      "1            2    Employee_2              1   52000\n",
      "2            3    Employee_3              1   50000\n",
      "3            4    Employee_4              2   60000\n",
      "4            5    Employee_5              2   61000\n",
      "\n",
      "Departments Data:\n",
      "   department_id department_name\n",
      "0              1     Engineering\n",
      "1              2       Marketing\n",
      "2              3         Finance\n",
      "3              4              HR\n",
      "4              5      Operations\n",
      "\n",
      "Department Average Salaries (using FROM subquery):\n",
      "  department_name  avg_salary\n",
      "0              HR     90600.0\n",
      "1         Finance     72000.0\n",
      "2       Marketing     60000.0\n",
      "3     Engineering     49000.0\n",
      "4      Operations     43000.0\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee data\n",
    "employees_data = {\n",
    "    'employee_id': range(1, 21),\n",
    "    'employee_name': [f\"Employee_{i}\" for i in range(1, 21)],\n",
    "    'department_id': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5],\n",
    "    'salary': [45000, 52000, 50000, 60000, 61000, 59000, 75000, 70000, 72000, 71000, \n",
    "               90000, 88000, 95000, 91000, 89000, 42000, 44000, 41000, 43000, 45000]\n",
    "}\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "\n",
    "# Create departments data\n",
    "departments_data = {\n",
    "    'department_id': range(1, 6),\n",
    "    'department_name': ['Engineering', 'Marketing', 'Finance', 'HR', 'Operations']\n",
    "}\n",
    "departments_df = pd.DataFrame(departments_data)\n",
    "\n",
    "# Display data\n",
    "print(\"Employees Data:\")\n",
    "print(employees_df.head())\n",
    "\n",
    "print(\"\\nDepartments Data:\")\n",
    "print(departments_df)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "employees_df.to_sql('employees', engine, index=False)\n",
    "departments_df.to_sql('departments', engine, index=False)\n",
    "\n",
    "# Query using a FROM clause subquery\n",
    "from_subquery = \"\"\"\n",
    "SELECT \n",
    "    d.department_name,\n",
    "    ds.avg_salary\n",
    "FROM \n",
    "    departments d\n",
    "JOIN \n",
    "    (SELECT \n",
    "        department_id,\n",
    "        AVG(salary) AS avg_salary\n",
    "     FROM \n",
    "        employees\n",
    "     GROUP BY \n",
    "        department_id) AS ds\n",
    "ON \n",
    "    d.department_id = ds.department_id\n",
    "ORDER BY \n",
    "    ds.avg_salary DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "results = pd.read_sql_query(from_subquery, engine)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nDepartment Average Salaries (using FROM subquery):\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b4160-6b11-42f1-a8bb-20bf35c00d8f",
   "metadata": {},
   "source": [
    "As promised in the first workshop, we can also use subqueries in combination with IN to check for membership against entire tables - usually the result of a subquery.\n",
    "\n",
    "For example, let's say that we want to select the names of all customers who have purchased on Electronic item, but the information between consumers, products and purchases are all on separate tables. We could first JOIN them, then use a WHERE statement to filter them. But this would create a very large merged table. Instead, we can just use an IN statement with a subquery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "656e9337-d9ab-4329-ba83-cf6c5217cea0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers:\n",
      "   customer_id customer_name\n",
      "0          101         Alice\n",
      "1          102           Bob\n",
      "2          103       Charlie\n",
      "3          104         David\n",
      "4          105          Emma\n",
      "\n",
      "Products:\n",
      "   product_id  product_name     category   price\n",
      "0           1        Laptop  Electronics  899.99\n",
      "1           2    Headphones  Electronics  149.99\n",
      "2           3  Coffee Maker   Appliances   89.99\n",
      "3           4       Blender   Appliances   59.99\n",
      "4           5    Smartphone  Electronics  699.99\n",
      "5           6    Desk Chair    Furniture  199.99\n",
      "6           7     Bookshelf    Furniture  129.99\n",
      "\n",
      "Purchases:\n",
      "   purchase_id  customer_id  product_id purchase_date\n",
      "0         1001          101           1    2023-01-15\n",
      "1         1002          101           5    2023-02-10\n",
      "2         1003          102           2    2023-01-22\n",
      "3         1004          103           3    2023-02-05\n",
      "4         1005          103           6    2023-03-12\n",
      "5         1006          104           4    2023-01-30\n",
      "6         1007          104           7    2023-02-28\n",
      "7         1008          105           5    2023-03-03\n",
      "8         1009          105           6    2023-03-18\n",
      "\n",
      "Customers who purchased Electronics:\n",
      "  customer_name\n",
      "0         Alice\n",
      "1           Bob\n",
      "2          Emma\n"
     ]
    }
   ],
   "source": [
    "# Create a database with customers and their purchases\n",
    "customers_data = {\n",
    "    'customer_id': [101, 102, 103, 104, 105],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma']\n",
    "}\n",
    "\n",
    "products_data = {\n",
    "    'product_id': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'product_name': ['Laptop', 'Headphones', 'Coffee Maker', 'Blender', 'Smartphone', 'Desk Chair', 'Bookshelf'],\n",
    "    'category': ['Electronics', 'Electronics', 'Appliances', 'Appliances', 'Electronics', 'Furniture', 'Furniture'],\n",
    "    'price': [899.99, 149.99, 89.99, 59.99, 699.99, 199.99, 129.99]\n",
    "}\n",
    "\n",
    "purchases_data = {\n",
    "    'purchase_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009],\n",
    "    'customer_id': [101, 101, 102, 103, 103, 104, 104, 105, 105],\n",
    "    'product_id': [1, 5, 2, 3, 6, 4, 7, 5, 6],\n",
    "    'purchase_date': ['2023-01-15', '2023-02-10', '2023-01-22', '2023-02-05', \n",
    "                      '2023-03-12', '2023-01-30', '2023-02-28', '2023-03-03', '2023-03-18']\n",
    "}\n",
    "\n",
    "# Create dataframes\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "products_df = pd.DataFrame(products_data)\n",
    "purchases_df = pd.DataFrame(purchases_data)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "customers_df.to_sql('customers', engine, index=False)\n",
    "products_df.to_sql('products', engine, index=False)\n",
    "purchases_df.to_sql('purchases', engine, index=False)\n",
    "\n",
    "# Display tables\n",
    "print(\"Customers:\")\n",
    "print(customers_df)\n",
    "print(\"\\nProducts:\")\n",
    "print(products_df)\n",
    "print(\"\\nPurchases:\")\n",
    "print(purchases_df)\n",
    "\n",
    "# Example 1: Using IN with a subquery to find customers who purchased Electronics\n",
    "electronics_query = \"\"\"\n",
    "SELECT  \n",
    "    c.customer_name\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.customer_id IN (\n",
    "        SELECT DISTINCT p.customer_id\n",
    "        FROM purchases p\n",
    "        JOIN products pr ON p.product_id = pr.product_id\n",
    "        WHERE pr.category = 'Electronics'\n",
    "    )\n",
    "ORDER BY\n",
    "    c.customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nCustomers who purchased Electronics:\")\n",
    "print(pd.read_sql_query(electronics_query, engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b769b",
   "metadata": {},
   "source": [
    "<a id='ctes'></a>\n",
    "## 4¬†¬∑¬†Common¬†Table¬†Expressions (CTEs) \n",
    "\n",
    "Subqueries are a great way of breaking down a complex query into smaller, more manageable subparts. But very often these subqueries can become quite long, and given that one must include the full query inside another query, they can become very hard to read. \n",
    "\n",
    "CTEs are a way of solving this issue. Instead of rewriting the entire subquery inside the main query, we first give aliases to our subqueries, and then refer to them in the main query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe5921-5505-443d-95cf-71fb99815845",
   "metadata": {},
   "source": [
    "\n",
    "Basic Syntax   \n",
    "\n",
    "```sql\n",
    "\n",
    "WITH cte_name AS (\n",
    "    SELECT column1, column2, ...\n",
    "    FROM table\n",
    "    WHERE condition)\n",
    "SELECT * \n",
    "FROM cte_name;\n",
    "Key Components\n",
    "```\n",
    "\n",
    "WITH clause: Introduces one or more CTEs\n",
    "cte_name: Gives a name to the temporary result set\n",
    "Main query: References the CTE like a regular table\n",
    "\n",
    "üîî **Question:** Would you say that CTEs improve readability? Can you think of an example in which the code is easier to read using subqueries instead?\n",
    "\n",
    "Multiple CTEs\n",
    "\n",
    "```sql\n",
    "\n",
    "WITH cte1 AS (\n",
    "    SELECT column1 FROM table1\n",
    "),\n",
    "cte2 AS (\n",
    "    SELECT column2 FROM table2\n",
    ")\n",
    "SELECT *\n",
    "FROM cte1\n",
    "JOIN cte2 ON cte1.column = cte2.column;\n",
    "```\n",
    "\n",
    "üí° **Tip:** CTE's can be referenced multiple times in the same query, which improve readability and prevents errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "defeea3d",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers who spend more than the average:\n",
      "   customer_id customer_name  total_spent  average_customer_spend\n",
      "0          101         Alice         1270                  586.25\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "orders_data = {\n",
    "    'order_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007],\n",
    "    'customer_id': [101, 101, 102, 101, 103, 102, 104],\n",
    "    'product': ['Laptop', 'Mouse', 'Monitor', 'Keyboard', 'Printer', 'Headphones', 'Tablet'],\n",
    "    'amount': [1200, 25, 350, 45, 180, 95, 450]\n",
    "}\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "\n",
    "customers_data = {\n",
    "    'customer_id': [101, 102, 103, 104],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "}\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "orders_df.to_sql('Orders', engine, index=False)\n",
    "customers_df.to_sql('Customers', engine, index=False)\n",
    "\n",
    "# Query using CTEs \n",
    "cte_query = \"\"\"\n",
    "WITH customer_totals AS (\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        SUM(amount) as total_spent\n",
    "    FROM \n",
    "        Orders\n",
    "    GROUP BY \n",
    "        customer_id\n",
    "),\n",
    "avg_spending AS (\n",
    "    SELECT \n",
    "        AVG(total_spent) as avg_spend\n",
    "    FROM \n",
    "        customer_totals\n",
    ")\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    ct.total_spent,\n",
    "    (SELECT avg_spend FROM avg_spending) as average_customer_spend\n",
    "FROM \n",
    "    Customers c\n",
    "JOIN \n",
    "    customer_totals ct ON c.customer_id = ct.customer_id\n",
    "WHERE \n",
    "    ct.total_spent > (SELECT avg_spend FROM avg_spending)\n",
    "ORDER BY\n",
    "    ct.total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(cte_query, engine)\n",
    "print(\"Customers who spend more than the average:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87d802-6a5f-4140-8cc4-9901cc65b6af",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** We cannot reference CTEs directly in a query - this is what we call (lack of) \"portability\".\n",
    "\n",
    "```sql\n",
    "WITH average_data AS (\n",
    "    SELECT AVG(value) AS avg_value FROM table\n",
    ")\n",
    "SELECT *\n",
    "FROM other_table\n",
    "WHERE value > average_data.avg_value \n",
    "```\n",
    "Whenever we want to reference a CTE, we must either use a subquery:\n",
    "\n",
    "```sql\n",
    "WHERE value > (SELECT avg_value FROM average_data)\n",
    "```\n",
    "\n",
    "Or JOIN with the CTE:\n",
    "\n",
    "```sql\n",
    "JOIN average_data ON 1=1\n",
    "WHERE value > average_data.avg_value\n",
    "```\n",
    "\n",
    "üí° **Tip:** In this last example, we used a common trick: choose an expression that is always true to add a constant column to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5ac0",
   "metadata": {},
   "source": [
    "<a id='pivot'></a>\n",
    "## 5¬†¬∑¬†Pivoting and Unpivoting (Or Melting) \n",
    "\n",
    "Pivoting is the process of turning rows into columns, Unpivoting (also called Melting) is the inverse process. \n",
    "\n",
    "A common application is when we would like to group information that applies to the same individual. For example, we might have a list of all the different transactions that different clients made, including amount and date. But maybe we would like to understand how purchases vary across the days of the week for each given customer. So we can turn a table from having many rows and three columns, to having one row for each consumer, and 8 columns: one representing the customer name (or any other identification), and one with the transaction amount for that customer on each day of the week. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6310f20-e0f7-482e-9b02-fb108c55722f",
   "metadata": {},
   "source": [
    "This is done by using the UNION (or UNION ALL) functions - which are analogues to JOINS, but instead of putting different columns side by side, they combine results from different queries on top of each other. \n",
    "\n",
    "The main difference between UNION and UNION ALL is that the former removes duplicate rows, which increases the computational cost of the function.\n",
    "\n",
    "Two additional observations:\n",
    "- The column names will be taken from the first one selected\n",
    "- All SELECT statements must have the same number of columns, and corresponding columns must have the same data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd85fc76-f262-4ceb-8811-34eef58e8412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    person  expenditure day_of_purchase\n",
      "0    Alice          100          Monday\n",
      "1    Alice          150         Tuesday\n",
      "2      Bob          200          Monday\n",
      "3      Bob          250       Wednesday\n",
      "4  Charlie          175         Tuesday\n",
      "5  Charlie          125          Friday\n",
      "6    Alice           90          Friday\n",
      "\n",
      "Pivoted Data:\n",
      "    person  Monday  Tuesday  Wednesday  Thursday  Friday\n",
      "0    Alice     100      150          0         0      90\n",
      "1      Bob     200        0        250         0       0\n",
      "2  Charlie       0      175          0         0     125\n"
     ]
    }
   ],
   "source": [
    "# Create sample data - expenditure by person and day\n",
    "expenditures_data = {\n",
    "    'person': ['Alice', 'Alice', 'Bob', 'Bob', 'Charlie', 'Charlie', 'Alice'],\n",
    "    'expenditure': [100, 150, 200, 250, 175, 125, 90],\n",
    "    'day_of_purchase': ['Monday', 'Tuesday', 'Monday', 'Wednesday', 'Tuesday', 'Friday', 'Friday']\n",
    "}\n",
    "expenditures_df = pd.DataFrame(expenditures_data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(expenditures_df)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "expenditures_df.to_sql('Expenditures', engine, index=False)\n",
    "\n",
    "# Pivot using UNION ALL\n",
    "pivot_query = \"\"\"\n",
    "SELECT person,\n",
    "       SUM(CASE WHEN day_of_purchase = 'Monday' THEN expenditure ELSE 0 END) AS Monday,\n",
    "       SUM(CASE WHEN day_of_purchase = 'Tuesday' THEN expenditure ELSE 0 END) AS Tuesday,\n",
    "       SUM(CASE WHEN day_of_purchase = 'Wednesday' THEN expenditure ELSE 0 END) AS Wednesday,\n",
    "       SUM(CASE WHEN day_of_purchase = 'Thursday' THEN expenditure ELSE 0 END) AS Thursday,\n",
    "       SUM(CASE WHEN day_of_purchase = 'Friday' THEN expenditure ELSE 0 END) AS Friday\n",
    "FROM Expenditures\n",
    "GROUP BY person\n",
    "\"\"\"\n",
    "\n",
    "# Execute query\n",
    "result = pd.read_sql_query(pivot_query, engine)\n",
    "print(\"\\nPivoted Data:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b69a3-735b-415d-ae8d-97dac450b683",
   "metadata": {},
   "source": [
    "Melting, the opposite process, is very useful when we want to do analysis regarding a variable that is not the one determining the rows. For example, say that we have sales data in which each row corresponds to a different product, and different columns represent different years. If we instead want to analyze the expenditure in different years, we can melt the table, and then use grouping or filtering to select a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bdaf070-917d-4960-b7b0-3dabfb81ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Wide Data:\n",
      "  product  sales_2021  sales_2022  sales_2023\n",
      "0  Laptop        1200        1500        1800\n",
      "1   Phone         800         950        1100\n",
      "2  Tablet         350         400         500\n",
      "\n",
      "Melted Data:\n",
      "  product  year  sales\n",
      "0  Laptop  2021   1200\n",
      "1  Laptop  2022   1500\n",
      "2  Laptop  2023   1800\n",
      "3   Phone  2021    800\n",
      "4   Phone  2022    950\n",
      "5   Phone  2023   1100\n",
      "6  Tablet  2021    350\n",
      "7  Tablet  2022    400\n",
      "8  Tablet  2023    500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create concise wide-format data with three years\n",
    "sales_data = {\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'sales_2021': [1200, 800, 350],\n",
    "    'sales_2022': [1500, 950, 400],\n",
    "    'sales_2023': [1800, 1100, 500]\n",
    "}\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "sales_df.to_sql('YearlySales', engine, index=False)\n",
    "\n",
    "# Melt the data using UNION ALL\n",
    "melt_query = \"\"\"\n",
    "SELECT product, '2021' AS year, sales_2021 AS sales FROM YearlySales\n",
    "UNION ALL\n",
    "SELECT product, '2022' AS year, sales_2022 AS sales FROM YearlySales\n",
    "UNION ALL\n",
    "SELECT product, '2023' AS year, sales_2023 AS sales FROM YearlySales\n",
    "ORDER BY product, year\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display results\n",
    "result = pd.read_sql_query(melt_query, engine)\n",
    "print(\"Original Wide Data:\")\n",
    "print(sales_df)\n",
    "print(\"\\nMelted Data:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c8c90",
   "metadata": {},
   "source": [
    "<a id='window'></a>\n",
    "## 6¬†¬∑¬†Window Functions \n",
    "**Definition.** A window is a subset of the table that is related in some prespecified way, in a very similar way that GROUP BY operates - with the crucial difference that it does not collapse the different rows into one, which allows us to preserve information. \n",
    "\n",
    "Window Functions operate separately into different windows. Common examples are\n",
    "\n",
    "- RANK - allow us to obtain the rank for different observations inside a given group. For example, we might want to see how much a consumer spent on his first purchase. So we would create a window for each consumer, apply the RANK window function, and then select those that have rank = 1.\n",
    "- DENSE_RANK - Similar to rank, but RANK jumps ranks if there are ties, while DENSE_RANK always has rankings as consecutive numbers. You might have seen this happening in college rankings!\n",
    "- ROW_NUMBER() - returns the row number of each observation. Very useful when the table lacks a primary key.\n",
    "- SUM - when used as a window function, allow us to perform cumulative sums - for example, cumulative sales up to a given date by each salesperson.\n",
    "- AVG/MAX/MIN - same as their aggregate versions, but allowing to keep information rather than collapsing rows.\n",
    "- LAG/LEAD - very useful in the context of time series, allow us to look at the previous/next value of a series.\n",
    "\n",
    "Basic Syntax: WINDOW_FUNCTION() OVER (PARTITION BY columns ORDER BY columns)\n",
    "\n",
    "Exception: SUM(column) OVER (PARTITION BY columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55f5cacb",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for all examples:\n",
      "  salesperson  quarter   sales\n",
      "0       Alice        1  120000\n",
      "1         Bob        1  120000\n",
      "2     Charlie        1   85000\n",
      "3       David        1   95000\n",
      "4        Emma        1   95000\n",
      "5       Alice        2  140000\n",
      "6         Bob        2  115000\n",
      "7     Charlie        2   90000\n",
      "8       David        2  100000\n",
      "9        Emma        2  100000\n",
      "...\n",
      "\n",
      "1. RANK Example - Annual Sales Rankings:\n",
      "  salesperson  annual_sales  sales_rank\n",
      "0       Alice        600000           1\n",
      "1         Bob        510000           2\n",
      "2        Emma        420000           3\n",
      "3       David        420000           3\n",
      "4     Charlie        375000           5\n",
      "\n",
      "2. DENSE_RANK Example - Quarterly Performance Rankings:\n",
      "    quarter salesperson   sales  quarter_rank\n",
      "0         1       Alice  120000             1\n",
      "1         1         Bob  120000             1\n",
      "2         1       David   95000             2\n",
      "3         1        Emma   95000             2\n",
      "4         1     Charlie   85000             3\n",
      "5         2       Alice  140000             1\n",
      "6         2         Bob  115000             2\n",
      "7         2       David  100000             3\n",
      "8         2        Emma  100000             3\n",
      "9         2     Charlie   90000             4\n",
      "10        3       Alice  160000             1\n",
      "11        3         Bob  125000             2\n",
      "12        3       David  110000             3\n",
      "13        3        Emma  110000             3\n",
      "14        3     Charlie   95000             4\n",
      "15        4       Alice  180000             1\n",
      "16        4         Bob  150000             2\n",
      "17        4       David  115000             3\n",
      "18        4        Emma  115000             3\n",
      "19        4     Charlie  105000             4\n",
      "\n",
      "3. ROW_NUMBER Example - Top Performer Each Quarter:\n",
      "   quarter salesperson   sales\n",
      "0        1       Alice  120000\n",
      "1        2       Alice  140000\n",
      "2        3       Alice  160000\n",
      "3        4       Alice  180000\n",
      "\n",
      "4. SUM() OVER Example - Running Sales Totals:\n",
      "   salesperson  quarter   sales  running_total\n",
      "0        Alice        1  120000         120000\n",
      "1        Alice        2  140000         260000\n",
      "2        Alice        3  160000         420000\n",
      "3        Alice        4  180000         600000\n",
      "4          Bob        1  120000         120000\n",
      "5          Bob        2  115000         235000\n",
      "6          Bob        3  125000         360000\n",
      "7          Bob        4  150000         510000\n",
      "8      Charlie        1   85000          85000\n",
      "9      Charlie        2   90000         175000\n",
      "10     Charlie        3   95000         270000\n",
      "11     Charlie        4  105000         375000\n",
      "12       David        1   95000          95000\n",
      "13       David        2  100000         195000\n",
      "14       David        3  110000         305000\n",
      "15       David        4  115000         420000\n",
      "16        Emma        1   95000          95000\n",
      "17        Emma        2  100000         195000\n",
      "18        Emma        3  110000         305000\n",
      "19        Emma        4  115000         420000\n",
      "\n",
      "5. LAG Example - Quarter-over-Quarter Growth:\n",
      "   salesperson  quarter   sales  prev_quarter_sales  growth_percent\n",
      "0        Alice        1  120000                 NaN             NaN\n",
      "1        Alice        2  140000            120000.0            16.7\n",
      "2        Alice        3  160000            140000.0            14.3\n",
      "3        Alice        4  180000            160000.0            12.5\n",
      "4          Bob        1  120000                 NaN             NaN\n",
      "5          Bob        2  115000            120000.0            -4.2\n",
      "6          Bob        3  125000            115000.0             8.7\n",
      "7          Bob        4  150000            125000.0            20.0\n",
      "8      Charlie        1   85000                 NaN             NaN\n",
      "9      Charlie        2   90000             85000.0             5.9\n",
      "10     Charlie        3   95000             90000.0             5.6\n",
      "11     Charlie        4  105000             95000.0            10.5\n",
      "12       David        1   95000                 NaN             NaN\n",
      "13       David        2  100000             95000.0             5.3\n",
      "14       David        3  110000            100000.0            10.0\n",
      "15       David        4  115000            110000.0             4.5\n",
      "16        Emma        1   95000                 NaN             NaN\n",
      "17        Emma        2  100000             95000.0             5.3\n",
      "18        Emma        3  110000            100000.0            10.0\n",
      "19        Emma        4  115000            110000.0             4.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a versatile sales dataset\n",
    "sales_data = {\n",
    "    'salesperson': ['Alice', 'Bob', 'Charlie', 'David', 'Emma',\n",
    "                   'Alice', 'Bob', 'Charlie', 'David', 'Emma',\n",
    "                   'Alice', 'Bob', 'Charlie', 'David', 'Emma',\n",
    "                   'Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
    "    'quarter': [1, 1, 1, 1, 1, \n",
    "               2, 2, 2, 2, 2, \n",
    "               3, 3, 3, 3, 3, \n",
    "               4, 4, 4, 4, 4],\n",
    "    'sales': [120000, 120000, 85000, 95000, 95000,\n",
    "             140000, 115000, 90000, 100000, 100000,\n",
    "             160000, 125000, 95000, 110000, 110000,\n",
    "             180000, 150000, 105000, 115000, 115000]\n",
    "}\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Create database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "sales_df.to_sql('SalesData', engine, index=False)\n",
    "\n",
    "print(\"Dataset for all examples:\")\n",
    "print(sales_df.head(10))\n",
    "print(\"...\")\n",
    "\n",
    "# Example 1: RANK - ranking salespeople by total annual sales\n",
    "rank_query = \"\"\"\n",
    "SELECT \n",
    "    salesperson,\n",
    "    SUM(sales) AS annual_sales,\n",
    "    RANK() OVER(ORDER BY SUM(sales) DESC) AS sales_rank\n",
    "FROM \n",
    "    SalesData\n",
    "GROUP BY \n",
    "    salesperson\n",
    "ORDER BY \n",
    "    sales_rank\n",
    "\"\"\"\n",
    "print(\"\\n1. RANK Example - Annual Sales Rankings:\")\n",
    "print(pd.read_sql_query(rank_query, engine))\n",
    "\n",
    "# Example 2: DENSE_RANK - ranking quarterly performance \n",
    "dense_rank_query = \"\"\"\n",
    "SELECT \n",
    "    quarter,\n",
    "    salesperson,\n",
    "    sales,\n",
    "    DENSE_RANK() OVER(PARTITION BY quarter ORDER BY sales DESC) AS quarter_rank\n",
    "FROM \n",
    "    SalesData\n",
    "ORDER BY \n",
    "    quarter, quarter_rank\n",
    "\"\"\"\n",
    "print(\"\\n2. DENSE_RANK Example - Quarterly Performance Rankings:\")\n",
    "print(pd.read_sql_query(dense_rank_query, engine))\n",
    "\n",
    "# Example 3: ROW_NUMBER - identifying top performer each quarter\n",
    "row_number_query = \"\"\"\n",
    "SELECT \n",
    "    quarter,\n",
    "    salesperson,\n",
    "    sales\n",
    "FROM (\n",
    "    SELECT \n",
    "        quarter,\n",
    "        salesperson,\n",
    "        sales,\n",
    "        ROW_NUMBER() OVER(PARTITION BY quarter ORDER BY sales DESC) AS row_num\n",
    "    FROM \n",
    "        SalesData\n",
    ") ranked\n",
    "WHERE \n",
    "    row_num = 1\n",
    "ORDER BY \n",
    "    quarter\n",
    "\"\"\"\n",
    "print(\"\\n3. ROW_NUMBER Example - Top Performer Each Quarter:\")\n",
    "print(pd.read_sql_query(row_number_query, engine))\n",
    "\n",
    "# Example 4: SUM() OVER - running total of sales per person\n",
    "sum_query = \"\"\"\n",
    "SELECT \n",
    "    salesperson,\n",
    "    quarter,\n",
    "    sales,\n",
    "    SUM(sales) OVER(PARTITION BY salesperson ORDER BY quarter) AS running_total\n",
    "FROM \n",
    "    SalesData\n",
    "ORDER BY \n",
    "    salesperson, quarter\n",
    "\"\"\"\n",
    "print(\"\\n4. SUM() OVER Example - Running Sales Totals:\")\n",
    "print(pd.read_sql_query(sum_query, engine))\n",
    "\n",
    "# Example 5: LAG - quarter-over-quarter growth\n",
    "lag_query = \"\"\"\n",
    "SELECT \n",
    "    salesperson,\n",
    "    quarter,\n",
    "    sales,\n",
    "    LAG(sales, 1) OVER(PARTITION BY salesperson ORDER BY quarter) AS prev_quarter_sales,\n",
    "    CASE \n",
    "        WHEN LAG(sales, 1) OVER(PARTITION BY salesperson ORDER BY quarter) IS NOT NULL\n",
    "        THEN ROUND((sales - LAG(sales, 1) OVER(PARTITION BY salesperson ORDER BY quarter)) * 100.0 / \n",
    "                  LAG(sales, 1) OVER(PARTITION BY salesperson ORDER BY quarter), 1)\n",
    "        ELSE NULL\n",
    "    END AS growth_percent\n",
    "FROM \n",
    "    SalesData\n",
    "ORDER BY \n",
    "    salesperson, quarter\n",
    "\"\"\"\n",
    "print(\"\\n5. LAG Example - Quarter-over-Quarter Growth:\")\n",
    "print(pd.read_sql_query(lag_query, engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5bb3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## 7¬†¬∑¬†Key Points <a id='keypoints'></a>\n",
    "- Joins combine tables using foreign keys.\n",
    "- Subqueries allow us to break down complex queries into more manageable parts.\n",
    "- CTEs make subqueries easier to read and re-use.\n",
    "- Pivoting and Melting allow us to reshape the table to better fit our purposes.\n",
    "- Window functions are an alternative to aggregate functions but still allow us to keep individual information.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
